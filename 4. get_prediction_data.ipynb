{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b643628",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from helpers import log, check_file_exists, load_csv, save_csv, remove_file_or_dir, calculate_season_average_until_gw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29853abd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/filtered_fixtures.csv\n"
     ]
    }
   ],
   "source": [
    "# This notebook lets us get csv with the data we want to predict\n",
    "data_directory = \"Fantasy-Premier-League/data/2024-25\"\n",
    "fixtures_file = os.path.join(data_directory, \"fixtures.csv\")\n",
    "teams_file = os.path.join(data_directory, \"teams.csv\")\n",
    "\n",
    "fixtures = load_csv(fixtures_file)\n",
    "teams = load_csv(teams_file)\n",
    "\n",
    "if fixtures is None or teams is None:\n",
    "    log(\"Failed to load required CSV files. Exiting.\", level=\"ERROR\")\n",
    "    exit()\n",
    "\n",
    "unfinished_fixtures = fixtures[~fixtures[\"finished\"]]\n",
    "next_6_gameweeks = unfinished_fixtures[\"event\"].dropna().unique()[:6]\n",
    "filtered_fixtures = unfinished_fixtures[unfinished_fixtures[\"event\"].isin(next_6_gameweeks)]\n",
    "\n",
    "team_columns = [\n",
    "    \"id\", \"short_name\", \"strength_attack_home\", \"strength_attack_away\",\n",
    "    \"strength_defence_home\", \"strength_defence_away\"\n",
    "]\n",
    "team_data = teams[team_columns]\n",
    "\n",
    "filtered_fixtures = filtered_fixtures.merge(\n",
    "    team_data,\n",
    "    left_on=\"team_a\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\"\n",
    ").rename(columns={\n",
    "    \"short_name\": \"short_name_a\",\n",
    "    \"strength_attack_away\": \"strength_attack_a\",\n",
    "    \"strength_defence_away\": \"strength_defense_a\"\n",
    "}).drop(columns=[\"strength_attack_home\", \"strength_defence_home\"])\n",
    "\n",
    "filtered_fixtures = filtered_fixtures.merge(\n",
    "    team_data,\n",
    "    left_on=\"team_h\",\n",
    "    right_on=\"id\",\n",
    "    how=\"left\",\n",
    "    suffixes=(\"\", \"_home\")\n",
    ").rename(columns={\n",
    "    \"short_name\": \"short_name_h\",\n",
    "    \"strength_attack_home\": \"strength_attack_h\",\n",
    "    \"strength_defence_home\": \"strength_defense_h\"\n",
    "})\n",
    "\n",
    "columns_to_keep = [\n",
    "    \"team_a\", \"team_h\", \"strength_attack_h\", \"strength_attack_a\",\n",
    "    \"strength_defense_h\", \"strength_defense_a\", \"short_name_h\", \n",
    "    \"short_name_a\", \"event\"\n",
    "]\n",
    "final_data = filtered_fixtures[columns_to_keep].rename(columns={\"event\": \"gameweek\"})\n",
    "\n",
    "output_file = os.path.join(data_directory, \"processed_data\", \"filtered_fixtures.csv\")\n",
    "save_csv(final_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8a66105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/players_with_clubs.csv\n"
     ]
    }
   ],
   "source": [
    "# Once we have that we can make players_with_clubs.csv where we will add current clubs of the\n",
    "# players and their values\n",
    "data_directory = \"Fantasy-Premier-League/data/2024-25/\"\n",
    "processed_players_file = os.path.join(data_directory, \"processed_data\", \"processed_players.csv\")\n",
    "players_raw_file = os.path.join(data_directory, \"players_raw.csv\")\n",
    "\n",
    "processed_players = load_csv(processed_players_file)\n",
    "players_raw = load_csv(players_raw_file)\n",
    "\n",
    "if processed_players is None or players_raw is None:\n",
    "    log(\"Failed to load player data. Exiting.\", level=\"ERROR\")\n",
    "    exit()\n",
    "\n",
    "merged_data = pd.merge(\n",
    "    processed_players,\n",
    "    players_raw[['id', 'now_cost', 'team', 'penalties_order']],\n",
    "    on='id',\n",
    "    how='left'\n",
    ").rename(columns={'now_cost': 'value', 'team': 'team_id'})\n",
    "\n",
    "output_file = os.path.join(data_directory, \"processed_data\", \"players_with_clubs.csv\")\n",
    "save_csv(merged_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "36eebe97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/GK/GK_with_clubs.csv\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/DEF/DEF_with_clubs.csv\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/MID/MID_with_clubs.csv\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/FWD/FWD_with_clubs.csv\n"
     ]
    }
   ],
   "source": [
    "# Let's split the data into positions now because it will make using different models easier for us later\n",
    "\n",
    "data_directory = \"Fantasy-Premier-League/data/2024-25/\"\n",
    "merged_file = os.path.join(data_directory, \"processed_data\", \"players_with_clubs.csv\")\n",
    "merged_data = load_csv(merged_file)\n",
    "\n",
    "if merged_data is None:\n",
    "    log(\"Failed to load merged player data. Exiting.\", level=\"ERROR\")\n",
    "    exit()\n",
    "\n",
    "position_folders = [\"GK\", \"DEF\", \"MID\", \"FWD\"]\n",
    "\n",
    "for position in position_folders:\n",
    "    position_data = merged_data[merged_data['position'] == position]\n",
    "    folder_path = os.path.join(data_directory, \"processed_data\", position)\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    output_file = os.path.join(folder_path, f\"{position}_with_clubs.csv\")\n",
    "    save_csv(position_data, output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3f1fa58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/GK/GK_with_features.csv\n",
      "INFO: Updated data saved to Fantasy-Premier-League/data/2024-25/processed_data/GK/GK_with_features.csv.\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/DEF/DEF_with_features.csv\n",
      "INFO: Updated data saved to Fantasy-Premier-League/data/2024-25/processed_data/DEF/DEF_with_features.csv.\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/MID/MID_with_features.csv\n",
      "INFO: Updated data saved to Fantasy-Premier-League/data/2024-25/processed_data/MID/MID_with_features.csv.\n",
      "INFO: Saved CSV: Fantasy-Premier-League/data/2024-25/processed_data/FWD/FWD_with_features.csv\n",
      "INFO: Updated data saved to Fantasy-Premier-League/data/2024-25/processed_data/FWD/FWD_with_features.csv.\n"
     ]
    }
   ],
   "source": [
    "def add_features(data_directory, positions):\n",
    "    \"\"\"\n",
    "    Add features required for predictions to player data for each position.\n",
    "    \"\"\"\n",
    "    for position in positions:\n",
    "        folder_path = os.path.join(data_directory, \"processed_data\", position)\n",
    "        position_file = os.path.join(folder_path, f\"{position}_with_clubs.csv\")\n",
    "\n",
    "        # Check if position file exists\n",
    "        if not check_file_exists(position_file):\n",
    "            log(f\"Position file {position_file} does not exist. Skipping.\", level=\"WARNING\")\n",
    "            continue\n",
    "\n",
    "        # Load the position-specific data\n",
    "        players_with_clubs = load_csv(position_file)\n",
    "        if players_with_clubs is None:\n",
    "            log(f\"Failed to load {position_file}. Skipping.\", level=\"WARNING\")\n",
    "            continue\n",
    "\n",
    "        updated_data = []\n",
    "\n",
    "        # Process each player's data\n",
    "        for _, player in players_with_clubs.iterrows():\n",
    "            player_id = player[\"id\"]\n",
    "            player_folder = find_player_folder(data_directory, player_id)\n",
    "\n",
    "            if not player_folder:\n",
    "                log(f\"Folder for player ID {player_id} not found. Skipping.\", level=\"WARNING\")\n",
    "                continue\n",
    "\n",
    "            gw_data = load_gw_data(player_folder)\n",
    "            if gw_data is None or gw_data.empty:\n",
    "                log(f\"GW file not found or empty for player ID {player_id}. Skipping.\", level=\"WARNING\")\n",
    "                continue\n",
    "            general_features = calculate_general_features(gw_data)\n",
    "            if position == \"GK\":\n",
    "                gk_features = calculate_gk_features(gw_data)\n",
    "                updated_player = {**player.to_dict(), **gk_features, **general_features}\n",
    "\n",
    "            updated_data.append(updated_player)\n",
    "\n",
    "        # Save the updated data\n",
    "        save_updated_data(folder_path, position, updated_data)\n",
    "\n",
    "\n",
    "def calculate_gk_features(gw_data):\n",
    "    \"\"\"\n",
    "    Calculate features required for goalkeeper prediction.\n",
    "    \"\"\"\n",
    "    rolling_periods = [3, 5, 8, 12]\n",
    "    required_columns = ['saves', 'clean_sheets', 'expected_goals_conceded', \n",
    "                        'total_points', 'minutes', 'bps', 'yellow_cards', 'penalties_saved']\n",
    "\n",
    "    # Ensure required columns are present\n",
    "    for column in required_columns:\n",
    "        if column not in gw_data.columns:\n",
    "            gw_data[column] = 0  # Initialize missing columns with 0\n",
    "\n",
    "    # Fill NaN values in existing columns\n",
    "    gw_data = gw_data.fillna(0)\n",
    "\n",
    "    # Sort the data by gameweek\n",
    "    gw_data = gw_data.sort_values(by=\"gameweek\").reset_index(drop=True)\n",
    "\n",
    "    gk_features = {}\n",
    "\n",
    "    # Calculate rolling features\n",
    "    for period in rolling_periods:\n",
    "        for feature in ['saves', 'expected_goals_conceded', \n",
    "                        'total_points', 'minutes', 'bps']:\n",
    "            gk_features[f\"rolling_{feature}_{period}\"] = (\n",
    "                gw_data[feature]\n",
    "                .rolling(window=period, min_periods=1)\n",
    "                .mean()\n",
    "                .iloc[-1] if not gw_data.empty else 0\n",
    "            )\n",
    "\n",
    "    # Calculate averages for specific stats only for filtered data\n",
    "    gk_features[\"avg_yellow_cards\"] = (\n",
    "        gw_data.loc[gw_data[\"minutes\"] > 0, \"yellow_cards\"].expanding().mean().iloc[-1]\n",
    "        if not gw_data.loc[gw_data[\"minutes\"] > 0].empty else 0\n",
    "    )\n",
    "\n",
    "    gk_features[\"avg_penalties_saved\"] = (\n",
    "        gw_data.loc[gw_data[\"minutes\"] > 0, \"penalties_saved\"].expanding().mean().iloc[-1]\n",
    "        if not gw_data.loc[gw_data[\"minutes\"] > 0].empty else 0\n",
    "    )\n",
    "\n",
    "    return gk_features\n",
    "\n",
    "def calculate_general_features(gw_data):\n",
    "    \"\"\"\n",
    "    Calculate features required for non-goalkeeper players.\n",
    "    \"\"\"\n",
    "    # Ensure pred_minutes is calculated\n",
    "    if \"pred_minutes\" not in gw_data.columns:\n",
    "        gw_data[\"pred_minutes\"] = gw_data[\"minutes\"].rolling(window=5, min_periods=1).apply(\n",
    "            lambda x: x[x != 0].iloc[-4:].mean() if len(x[x != 0]) > 0 else 0, raw=False\n",
    "        )\n",
    "\n",
    "    # Example of adding pred_minutes to non-goalkeepers\n",
    "    general_features = {}\n",
    "    general_features[\"pred_minutes\"] = gw_data[\"pred_minutes\"].iloc[-1] if not gw_data.empty else 0\n",
    "    return general_features\n",
    "\n",
    "def find_player_folder(data_directory, player_id):\n",
    "    \"\"\"\n",
    "    Locate the folder for a specific player by ID.\n",
    "    \"\"\"\n",
    "    players_dir = os.path.join(data_directory, \"players\")\n",
    "    return next(\n",
    "        (os.path.join(players_dir, folder) for folder in os.listdir(players_dir)\n",
    "         if folder.endswith(f\"_{player_id}\")),\n",
    "        None\n",
    "    )\n",
    "\n",
    "def load_gw_data(player_folder):\n",
    "    \"\"\"\n",
    "    Load the player's gameweek data (gw.csv) from their folder.\n",
    "    \"\"\"\n",
    "    gw_file = os.path.join(player_folder, \"gw.csv\")\n",
    "    if not check_file_exists(gw_file, log_missing=True):\n",
    "        return None\n",
    "    return load_csv(gw_file)\n",
    "\n",
    "def save_updated_data(folder_path, position, updated_data):\n",
    "    \"\"\"\n",
    "    Save the updated data for a position to a CSV file.\n",
    "    \"\"\"\n",
    "    if not updated_data:\n",
    "        log(f\"No data to save for position {position}. Skipping.\", level=\"INFO\")\n",
    "        return\n",
    "\n",
    "    output_file = os.path.join(folder_path, f\"{position}_with_features.csv\")\n",
    "    save_csv(pd.DataFrame(updated_data), output_file)\n",
    "    log(f\"Updated data saved to {output_file}.\", level=\"INFO\")\n",
    "\n",
    "data_directory = \"Fantasy-Premier-League/data/2024-25\"\n",
    "positions = [\"GK\", \"DEF\", \"MID\", \"FWD\"]\n",
    "add_features(data_directory, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbf85c4e-6837-4e39-884d-9ea713c39867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_xg_prediction_features(gw_data):\n",
    "#     \"\"\"\n",
    "#     Calculate features required for xG prediction from gameweek data.\n",
    "#     \"\"\"\n",
    "#     # Calculate pred_minutes\n",
    "#     gw_data[\"pred_minutes\"] = gw_data[\"minutes\"].rolling(window=5, min_periods=1).apply(\n",
    "#         lambda x: x[x != 0].iloc[-4:].mean() if len(x[x != 0]) > 0 else 0, raw=False\n",
    "#     )\n",
    "    \n",
    "#     cumulative_npxg = gw_data[\"npxG\"].sum()\n",
    "#     cumulative_npg = gw_data[\"npg\"].sum()\n",
    "\n",
    "#     # Create a filtered DataFrame for calculations requiring minutes >= 60\n",
    "#     gw_data_filtered = gw_data.loc[gw_data[\"minutes\"] >= 60].copy()\n",
    "#     if gw_data_filtered.empty:  # Early exit if filtered data is empty\n",
    "#         return {\n",
    "#             \"pred_minutes\": gw_data[\"pred_minutes\"].iloc[-1] if not gw_data.empty else 0,\n",
    "#             \"rolling_adjxg_5\": 0,\n",
    "#             \"rolling_shots_5\": 0,\n",
    "#             \"season_mean_bonus\": 0,\n",
    "#             \"average_adjxg_season\": 0,\n",
    "#             \"average_shots_season\": 0,\n",
    "#             \"cumulative_npxg\": cumulative_npxg,\n",
    "#             \"cumulative_npg\": cumulative_npg,\n",
    "#         }\n",
    "#     gw_data_filtered[\"rolling_adjxg_5\"] = gw_data_filtered[\"adjusted_xg\"].rolling(window=5, min_periods=1).mean()\n",
    "#     gw_data_filtered[\"rolling_shots_5\"] = gw_data_filtered[\"shots\"].rolling(window=5, min_periods=1).mean()\n",
    "#     gw_data_filtered[\"season_mean_bonus\"] = gw_data_filtered[\"bonus\"].expanding(min_periods=1).mean()\n",
    "#     gw_data_filtered[\"average_adjxg_season\"] = gw_data_filtered[\"adjusted_xg\"].expanding(min_periods=1).mean()\n",
    "#     gw_data_filtered[\"average_shots_season\"] = gw_data_filtered[\"shots\"].expanding(min_periods=1).mean()\n",
    "\n",
    "#     return {\n",
    "#         \"pred_minutes\": gw_data[\"pred_minutes\"].iloc[-1] if not gw_data.empty else 0,\n",
    "#         \"rolling_adjxg_5\": gw_data_filtered[\"rolling_adjxg_5\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"rolling_shots_5\": gw_data_filtered[\"rolling_shots_5\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"season_mean_bonus\": gw_data_filtered[\"season_mean_bonus\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"average_adjxg_season\": gw_data_filtered[\"average_adjxg_season\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"average_shots_season\": gw_data_filtered[\"average_shots_season\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"cumulative_npxg\": cumulative_npxg,\n",
    "#         \"cumulative_npg\": cumulative_npg\n",
    "#     }\n",
    "\n",
    "# def calculate_xa_prediction_features(gw_data):\n",
    "#     \"\"\"\n",
    "#     Calculate assist-related features.\n",
    "#     \"\"\"\n",
    "#     cumulative_xa = gw_data[\"expected_assists\"].sum()\n",
    "#     cumulative_assists = gw_data[\"assists\"].sum()\n",
    "    \n",
    "#     gw_data_filtered = gw_data.loc[gw_data[\"minutes\"] >= 60].copy()\n",
    "\n",
    "#     if gw_data_filtered.empty:  # Early exit if filtered data is empty\n",
    "#         return {\n",
    "#             \"rolling_xa_5\": 0,\n",
    "#             \"rolling_key_passes_5\": 0,\n",
    "#             \"average_xa_per_game\": 0,\n",
    "#             \"average_key_passes_per_game\": 0,\n",
    "#             \"cumulative_xa\": cumulative_xa,\n",
    "#             \"cumulative_assists\": cumulative_assists,\n",
    "#         }\n",
    "\n",
    "    \n",
    "#     gw_data_filtered[\"rolling_xa_5\"] = gw_data_filtered[\"expected_assists\"].rolling(window=5, min_periods=1).mean()\n",
    "#     gw_data_filtered[\"rolling_key_passes_5\"] = gw_data_filtered[\"key_passes\"].rolling(window=5, min_periods=1).mean()\n",
    "#     gw_data_filtered[\"average_xa_season\"] = gw_data_filtered[\"expected_assists\"].expanding(min_periods=1).mean()\n",
    "#     gw_data_filtered[\"average_key_passes_season\"] = gw_data_filtered[\"key_passes\"].expanding(min_periods=1).mean()\n",
    "    \n",
    "#     return {\n",
    "#         \"rolling_xa_5\": gw_data_filtered[\"rolling_xa_5\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"rolling_key_passes_5\": gw_data_filtered[\"rolling_key_passes_5\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"average_xa_season\": gw_data_filtered[\"average_xa_season\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"average_key_passes_season\": gw_data_filtered[\"average_key_passes_season\"].iloc[-1] if not gw_data_filtered.empty else 0,\n",
    "#         \"cumulative_xa\": cumulative_xa,\n",
    "#         \"cumulative_assists\": cumulative_assists\n",
    "#     }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6029de40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Saved merged data for GK to Fantasy-Premier-League/data/2024-25/processed_data/GK/GK_final.csv\n",
      "INFO: Saved merged data for DEF to Fantasy-Premier-League/data/2024-25/processed_data/DEF/DEF_final.csv\n",
      "INFO: Saved merged data for MID to Fantasy-Premier-League/data/2024-25/processed_data/MID/MID_final.csv\n",
      "INFO: Saved merged data for FWD to Fantasy-Premier-League/data/2024-25/processed_data/FWD/FWD_final.csv\n"
     ]
    }
   ],
   "source": [
    "def merge_with_fixtures(data_directory, fixtures_file, positions):\n",
    "    \"\"\"\n",
    "    Merge player data with fixtures for each position and save the final merged data.\n",
    "    \"\"\"\n",
    "    fixtures = pd.read_csv(fixtures_file)\n",
    "    position_mapping = {\"GK\": 1, \"DEF\": 2, \"MID\": 3, \"FWD\": 4}\n",
    "\n",
    "    required_prefixes = [\"rolling_\", \"average_\", \"cumulative_\", \"avg_\"]\n",
    "    required_columns = [\n",
    "        \"id\", \"first_name\", \"second_name\", \"own_team\", \"opponent_team\",\n",
    "        \"own_short_name\", \"opponent_short_name\", \"own_attack\", \"opponent_attack\",\n",
    "        \"own_defense\", \"opponent_defense\", \"was_home\", \"season\", \"POS\",\n",
    "        \"gameweek\", \"value\", \"pred_minutes\", 'penalties_order'\n",
    "    ]\n",
    "\n",
    "    for position in positions:\n",
    "        folder_path = os.path.join(data_directory, \"processed_data\", position)\n",
    "        position_file = os.path.join(folder_path, f\"{position}_with_features.csv\")\n",
    "\n",
    "        if not os.path.exists(position_file):\n",
    "            print(f\"{position_file} not found. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        # Load player data\n",
    "        players = pd.read_csv(position_file)\n",
    "\n",
    "        # Process and merge data for home and away fixtures\n",
    "        home_merge = process_fixtures_merge(players, fixtures, \"team_h\", \"team_a\", was_home=1)\n",
    "        away_merge = process_fixtures_merge(players, fixtures, \"team_a\", \"team_h\", was_home=0)\n",
    "\n",
    "        # Concatenate home and away data\n",
    "        merged = pd.concat([home_merge, away_merge], ignore_index=True)\n",
    "\n",
    "        # Add additional columns\n",
    "        merged[\"season\"] = 24  # Fixed season value\n",
    "        merged[\"POS\"] = position_mapping[position]  # Position mapping\n",
    "\n",
    "        all_columns = merged.columns\n",
    "        dynamic_columns_to_keep = [\n",
    "            col for col in all_columns\n",
    "            if col in required_columns or any(col.startswith(prefix) for prefix in required_prefixes)\n",
    "        ]\n",
    "\n",
    "        final_data = merged[dynamic_columns_to_keep]\n",
    "\n",
    "        # Save the merged data\n",
    "        output_file = os.path.join(folder_path, f\"{position}_final.csv\")\n",
    "        final_data.to_csv(output_file, index=False)\n",
    "        log(f\"Saved merged data for {position} to {output_file}\", level=\"INFO\")\n",
    "\n",
    "def process_fixtures_merge(players, fixtures, own_key, opponent_key, was_home):\n",
    "    \"\"\"\n",
    "    Helper function to process the merge between players and fixtures.\n",
    "    \"\"\"\n",
    "    merged = players.merge(\n",
    "        fixtures,\n",
    "        left_on=\"team_id\",\n",
    "        right_on=own_key,\n",
    "        how=\"inner\"\n",
    "    )\n",
    "    merged[\"was_home\"] = was_home\n",
    "\n",
    "    # Rename columns for consistency\n",
    "    column_mapping = {\n",
    "        own_key: \"own_team\",\n",
    "        opponent_key: \"opponent_team\",\n",
    "        f\"short_name_{own_key[-1]}\": \"own_short_name\",\n",
    "        f\"short_name_{opponent_key[-1]}\": \"opponent_short_name\",\n",
    "        f\"strength_attack_{own_key[-1]}\": \"own_attack\",\n",
    "        f\"strength_defense_{own_key[-1]}\": \"own_defense\",\n",
    "        f\"strength_attack_{opponent_key[-1]}\": \"opponent_attack\",\n",
    "        f\"strength_defense_{opponent_key[-1]}\": \"opponent_defense\"\n",
    "    }\n",
    "    merged.rename(columns=column_mapping, inplace=True)\n",
    "    return merged\n",
    "\n",
    "\n",
    "# Main Execution\n",
    "data_directory = \"Fantasy-Premier-League/data/2024-25/\"\n",
    "fixtures_file = os.path.join(data_directory, \"processed_data\", \"filtered_fixtures.csv\")\n",
    "positions = [\"GK\", \"DEF\", \"MID\", \"FWD\"]\n",
    "merge_with_fixtures(data_directory, fixtures_file, positions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947b6f8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "tvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
