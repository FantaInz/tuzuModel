{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "31fd57ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from rapidfuzz import fuzz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62a887bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get each player position and id for the season\n",
    "def process_season_data(data_directory, seasons):\n",
    "    for season in seasons:\n",
    "        season_path = os.path.join(data_directory, season)\n",
    "\n",
    "        if os.path.isdir(season_path):\n",
    "            player_idlist_path = os.path.join(season_path, \"player_idlist.csv\")\n",
    "            cleaned_players_path = os.path.join(season_path, \"cleaned_players.csv\")\n",
    "            output_file_path = os.path.join(season_path, \"processed_players.csv\")\n",
    "\n",
    "            if os.path.exists(player_idlist_path) and os.path.exists(cleaned_players_path):\n",
    "                try:\n",
    "                    player_idlist_df = pd.read_csv(player_idlist_path)\n",
    "                    cleaned_players_df = pd.read_csv(cleaned_players_path)\n",
    "\n",
    "                    merged_df = pd.merge(\n",
    "                        player_idlist_df[['id', 'first_name', 'second_name']],\n",
    "                        cleaned_players_df[['first_name', 'second_name', 'element_type']],\n",
    "                        on=['first_name', 'second_name'],\n",
    "                        how='inner'\n",
    "                    )\n",
    "\n",
    "                    # Rename element_type to position\n",
    "                    merged_df.rename(columns={'element_type': 'position'}, inplace=True)\n",
    "\n",
    "                    # Save the DataFrame to a new file\n",
    "                    merged_df.to_csv(output_file_path, index=False)\n",
    "                    print(f\"Processed and saved: {output_file_path}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing season {season}: {e}\")\n",
    "            else:\n",
    "                print(f\"Missing required files in {season_path}: player_idlist.csv or cleaned_players.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e5c25904",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: Fantasy-Premier-League/data/2024-25/processed_players.csv\n",
      "Processed and saved: Fantasy-Premier-League/data/2023-24/processed_players.csv\n",
      "Processed and saved: Fantasy-Premier-League/data/2022-23/processed_players.csv\n",
      "Processed and saved: Fantasy-Premier-League/data/2021-22/processed_players.csv\n",
      "Processed and saved: Fantasy-Premier-League/data/2020-21/processed_players.csv\n"
     ]
    }
   ],
   "source": [
    "data_directory = \"Fantasy-Premier-League/data\"\n",
    "seasons = [\"2024-25\", \"2023-24\", \"2022-23\", \"2021-22\", \"2020-21\"]\n",
    "process_season_data(data_directory, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea403c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_player_ids(data_directory, seasons, output_file=\"master_player_list.csv\"):\n",
    "    player_data = {}\n",
    "    next_unique_id = 1\n",
    "    \n",
    "    # We want to base ids on 2024-25 season\n",
    "    main_season = \"2024-25\"\n",
    "    main_season_path = os.path.join(data_directory, main_season, \"processed_players.csv\")\n",
    "    \n",
    "    if os.path.exists(main_season_path):\n",
    "        try:\n",
    "            main_processed_df = pd.read_csv(main_season_path)\n",
    "            \n",
    "            for _, row in main_processed_df.iterrows():\n",
    "                full_name = f\"{row['first_name']} {row['second_name']}\"\n",
    "                if full_name not in player_data:\n",
    "                    player_data[full_name] = {\n",
    "                        \"First_Name\": row['first_name'],\n",
    "                        \"Last_Name\": row['second_name'],\n",
    "                        \"Unique_ID\": row['id']\n",
    "                    }\n",
    "                # Add the 24_id for the main season\n",
    "                player_data[full_name]['24_id'] = row['id']\n",
    "                next_unique_id = max(next_unique_id, row['id'] + 1)  # Ensure greater ids to avoid reusing\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {main_season_path}: {e}\")\n",
    "    else:\n",
    "        print(f\"Missing processed_players.csv for season: {main_season}\")\n",
    "\n",
    "    # Process other seasons\n",
    "    for season in seasons:\n",
    "        \n",
    "        if season != main_season:\n",
    "            season_path = os.path.join(data_directory, season, \"processed_players.csv\")\n",
    "            if os.path.exists(season_path):\n",
    "                try:\n",
    "                    processed_df = pd.read_csv(season_path)\n",
    "                    \n",
    "                    season_short = season[:4][-2:]\n",
    "                    \n",
    "                    for _, row in processed_df.iterrows():\n",
    "                        full_name = f\"{row['first_name']} {row['second_name']}\"\n",
    "                        if full_name not in player_data:\n",
    "                            player_data[full_name] = {\n",
    "                                \"First_Name\": row['first_name'],\n",
    "                                \"Last_Name\": row['second_name'],\n",
    "                                \"Unique_ID\": next_unique_id\n",
    "                            }\n",
    "                            next_unique_id += 1\n",
    "                        # Add the season ID\n",
    "                        player_data[full_name][f\"{season_short}_id\"] = row['id']\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {season_path}: {e}\")\n",
    "\n",
    "    consolidated_df = pd.DataFrame.from_dict(player_data, orient='index').reset_index(drop=True)\n",
    "    \n",
    "    # Ensure all ID columns are integers\n",
    "    id_columns = [col for col in consolidated_df.columns if col.endswith(\"_id\")]\n",
    "    consolidated_df[id_columns] = consolidated_df[id_columns].fillna(-1).astype(int)\n",
    "    \n",
    "    # Save the consolidated DataFrame to a CSV\n",
    "    output_path = os.path.join(data_directory, output_file)\n",
    "    consolidated_df.to_csv(output_path, index=False)\n",
    "    print(f\"Consolidated player data saved to {output_path}\")\n",
    "\n",
    "    return consolidated_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994fc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Consolidated player data saved to Fantasy-Premier-League/data/master_player_list.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First_Name</th>\n",
       "      <th>Last_Name</th>\n",
       "      <th>Unique_ID</th>\n",
       "      <th>24_id</th>\n",
       "      <th>23_id</th>\n",
       "      <th>22_id</th>\n",
       "      <th>21_id</th>\n",
       "      <th>20_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fábio</td>\n",
       "      <td>Ferreira Vieira</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Gabriel</td>\n",
       "      <td>Fernando de Jesus</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>28</td>\n",
       "      <td>263</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Gabriel</td>\n",
       "      <td>dos Santos Magalhães</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Kai</td>\n",
       "      <td>Havertz</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>145</td>\n",
       "      <td>141</td>\n",
       "      <td>500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Karl</td>\n",
       "      <td>Hein</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>646</td>\n",
       "      <td>655</td>\n",
       "      <td>532</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1748</th>\n",
       "      <td>Theo</td>\n",
       "      <td>Corbeanu</td>\n",
       "      <td>1749</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1749</th>\n",
       "      <td>Taylor</td>\n",
       "      <td>Perry</td>\n",
       "      <td>1750</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1750</th>\n",
       "      <td>Lewis</td>\n",
       "      <td>Richards</td>\n",
       "      <td>1751</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1751</th>\n",
       "      <td>Nigel</td>\n",
       "      <td>Lonwijk</td>\n",
       "      <td>1752</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1752</th>\n",
       "      <td>Willian José</td>\n",
       "      <td>Da Silva</td>\n",
       "      <td>1753</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>-1</td>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1753 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        First_Name             Last_Name  Unique_ID  24_id  23_id  22_id  \\\n",
       "0            Fábio       Ferreira Vieira          1      1      4     25   \n",
       "1          Gabriel     Fernando de Jesus          2      2      8     28   \n",
       "2          Gabriel  dos Santos Magalhães          3      3      5     16   \n",
       "3              Kai               Havertz          4      4      6    145   \n",
       "4             Karl                  Hein          5      5    646    655   \n",
       "...            ...                   ...        ...    ...    ...    ...   \n",
       "1748          Theo              Corbeanu       1749     -1     -1     -1   \n",
       "1749        Taylor                 Perry       1750     -1     -1     -1   \n",
       "1750         Lewis              Richards       1751     -1     -1     -1   \n",
       "1751         Nigel               Lonwijk       1752     -1     -1     -1   \n",
       "1752  Willian José              Da Silva       1753     -1     -1     -1   \n",
       "\n",
       "      21_id  20_id  \n",
       "0        -1     -1  \n",
       "1       263    282  \n",
       "2        -1     -1  \n",
       "3       141    500  \n",
       "4       532     -1  \n",
       "...     ...    ...  \n",
       "1748     -1    610  \n",
       "1749     -1    611  \n",
       "1750     -1    615  \n",
       "1751     -1    629  \n",
       "1752     -1    642  \n",
       "\n",
       "[1753 rows x 8 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merge_player_ids(data_directory, seasons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa790f8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Potential renamed players saved to created_csv/players_with_potential_renames.csv\n"
     ]
    }
   ],
   "source": [
    "# Script below was used to get duplicate candidates\n",
    "\n",
    "consolidated_file = \"master_player_list.csv\"\n",
    "consolidated_path = os.path.join(data_directory, consolidated_file)\n",
    "consolidated_df = pd.read_csv(consolidated_path)\n",
    "\n",
    "SIMILARITY_THRESHOLD = 80\n",
    "FIRST_NAME_SIMILARITY_THRESHOLD = 40\n",
    "\n",
    "def is_pair(player_a, player_b, id_columns):\n",
    "    # Check for ID overlaps across seasons\n",
    "    id_overlap = any(\n",
    "        player_a[col] != -1 and player_b[col] != -1\n",
    "        for col in id_columns\n",
    "    )\n",
    "    if id_overlap:\n",
    "        return False\n",
    "    \n",
    "    # Now we check the names similarity\n",
    "    first_name_similarity = fuzz.partial_ratio(player_a[\"First_Name\"], player_b[\"First_Name\"])\n",
    "    if first_name_similarity <= FIRST_NAME_SIMILARITY_THRESHOLD:\n",
    "        return False\n",
    "    \n",
    "    # Check if one last name is contained in the other\n",
    "    if player_a[\"Last_Name\"] in player_b[\"Last_Name\"] or player_b[\"Last_Name\"] in player_a[\"Last_Name\"]:\n",
    "        return True\n",
    "\n",
    "    # Calculate string similarity only if containment check fails\n",
    "    similarity = fuzz.partial_ratio(player_a[\"Last_Name\"], player_b[\"Last_Name\"])\n",
    "    return similarity >= SIMILARITY_THRESHOLD\n",
    "\n",
    "# Identify all ID columns\n",
    "id_columns = [col for col in consolidated_df.columns if col.endswith(\"_id\")]\n",
    "pairs = []\n",
    "for i, player_a in consolidated_df.iterrows():\n",
    "    for j, player_b in consolidated_df.iterrows():\n",
    "        if i >= j:  # Avoid duplicate and self-pairs\n",
    "            continue\n",
    "        if is_pair(player_a, player_b, id_columns):\n",
    "            pairs.append({\n",
    "                \"Player_A_First_Name\": player_a[\"First_Name\"],\n",
    "                \"Player_A_Last_Name\": player_a[\"Last_Name\"],\n",
    "                \"Player_A_Unique_ID\": player_a[\"Unique_ID\"],\n",
    "                \"Player_B_First_Name\": player_b[\"First_Name\"],\n",
    "                \"Player_B_Last_Name\": player_b[\"Last_Name\"],\n",
    "                \"Player_B_Unique_ID\": player_b[\"Unique_ID\"],\n",
    "            })\n",
    "\n",
    "# Convert pairs to a DataFrame\n",
    "created_csv_dir = \"created_csv\"\n",
    "pairs_df = pd.DataFrame(pairs)\n",
    "if not pairs_df.empty:\n",
    "    output_path = os.path.join(created_csv_dir, \"players_with_potential_renames.csv\")\n",
    "    pairs_df.to_csv(output_path, index=False)\n",
    "    print(f\"Potential renamed players saved to {output_path}\")\n",
    "else:\n",
    "    print(\"No potential renamed players found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4a78425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After manual checking the candidates now we have to\n",
    "# - adjust their "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e0574d57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57.14285714285714"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fuzz.partial_ratio(\"Edward\", \"Eddie\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a54bf6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tvenv",
   "language": "python",
   "name": "tvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
